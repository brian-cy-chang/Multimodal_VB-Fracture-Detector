[general]
seed=23

# train or predict
mode = train
output_path = ./output

# use-specified folder name. If not provided, default name is <model_name>_timestamp_<bert_mode>
output_folder = 

# specify a model architecture name to use, must be one in joint_fusion or late_fusion
model_name = JointFusion_CNN
use_fine_tuned = False

# path of fine-tune model that must be a .pth file, applicable only if use_fine_tuned = True
fine_tuned_path =

# name of model.state_dict to save to .pth if mode = train. If not set, default name is <model_name>_<loss_function>_timestamp
save_name = 

[bert]
# discrete or cls
bert_mode = discrete
# if bert_mode = discrete, set max padding length
discrete_max_length_pad = 6
# if preprocess_mode = cls
cls_max_length_pad = 200

# if bert_mode = discrete, must be pickle file
train_discrete_file = /data2//bcc92/multimodal/data/UWspine_train_discrete_events_corrected.pkl
validation_discrete_file = /data2//bcc92/multimodal/data/UWspine_validation_discrete_events_corrected.pkl
predict_discrete_file = /data2//bcc92/multimodal/data/UWspine_test_discrete_events_corrected.pkl

train_cls_file = /data2/bcc92/multimodal/data/train_subject_CLS_ensemble_corrected.pkl
validation_cls_file = /data2/bcc92/multimodal/data/validation_subject_CLS_ensemble_corrected.pkl
predict_cls_file = /data2/bcc92/multimodal/data/test_subject_CLS_ensemble_corrected.pkl

[vb]
train_path = /data2/bcc92/os_pipeline/output/train/vb_classification2
validation_path = /data2/bcc92/os_pipeline/output/validation/vb_classification2
predict_path = /data2/bcc92/os_pipeline/output/test/vb_classification2

[patient]
# must be csv
pt_dem_file = /data2/bcc92/multimodal/data/images_demographics_amended.csv

[labels]
# must be csv
train_label_file = /data2/bcc92/uw_dataset_related/UW_m2ABQ/train.csv
validation_label_file = /data2/bcc92/uw_dataset_related/UW_m2ABQ/valid.csv
test_label_file = /data2/bcc92/uw_dataset_related/UW_m2ABQ/test.csv
groundtruth_file = /data2/bcc92/uw_dataset_related/UW_m2ABQ/annotations.csv

[preprocess]
# onehot or label
encode_mode = onehot

[model]
# bce (binary cross-entropy) or focal
loss_function = bce
batch_size = 4 
# kaiming, xavier, he, or None (leave blank)
weight_init =  
epochs = 200 
# hidden_size = 256
# if none, will use default values 
focal_loss_alpha = 0.25
focal_loss_gamma = 2
learning_rate = 0.001 
weight_decay = 0.01
momentum = 0.9  
threshold = 0.0 
patience = 10
dropout_rate = 0.5
grad_clipping = 1.0